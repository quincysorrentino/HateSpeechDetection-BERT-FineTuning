{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e45db77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, f1_score\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "from typing import Dict, List, Tuple, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "550bdc1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(64001, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(130, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load model and tokenizer\n",
    "model_path = \"../hate_speech_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, local_files_only=True)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6920b2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Label mappings\n",
    "label_map = {0: \"hatespeech\", 1: \"offensive\", 2: \"normal\"}\n",
    "inv_label_map = {v: k for k, v in label_map.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9974de84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text by removing non-ASCII characters and normalizing whitespace\"\"\"\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def get_probabilities(texts: List[str], labels: List[int], model, tokenizer) -> Tuple[np.ndarray, List[int]]:\n",
    "    \"\"\"Get model probabilities for all texts, handling errors gracefully\"\"\"\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    new_labels = []\n",
    "    skipped = 0\n",
    "    \n",
    "    for i, (text, label) in enumerate(zip(texts, labels)):\n",
    "        try:\n",
    "            if not isinstance(text, str):\n",
    "                text = str(text)\n",
    "            text = clean_text(text)\n",
    "            \n",
    "            # Skip empty texts\n",
    "            if not text.strip():\n",
    "                print(f\"⚠️ Skipping empty text at index {i}\")\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "            with torch.no_grad():\n",
    "                logits = model(**inputs).logits\n",
    "                probs = F.softmax(logits, dim=1).squeeze()\n",
    "                all_probs.append(probs.cpu().numpy())\n",
    "                new_labels.append(label)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Skipping example {i}: {repr(text[:50])} -> {e}\")\n",
    "            skipped += 1\n",
    "    \n",
    "    print(f\"✅ Processed {len(all_probs)} examples. Skipped {skipped} problematic examples.\")\n",
    "    return np.array(all_probs), new_labels\n",
    "\n",
    "def optimize_confidence_threshold(probs: np.ndarray, true_labels: List[int], \n",
    "                                metric: str = \"f1\") -> float:\n",
    "    \"\"\"\n",
    "    Optimize a single confidence threshold where predictions below threshold are marked as uncertain.\n",
    "    This is useful when you want to identify low-confidence predictions.\n",
    "    \"\"\"\n",
    "    thresholds = np.arange(0.1, 1.0, 0.01)\n",
    "    best_score = 0\n",
    "    best_threshold = 0.5\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        predictions = []\n",
    "        valid_true = []\n",
    "        \n",
    "        for i, prob in enumerate(probs):\n",
    "            max_prob = np.max(prob)\n",
    "            if max_prob >= threshold:\n",
    "                predictions.append(np.argmax(prob))\n",
    "                valid_true.append(true_labels[i])\n",
    "        \n",
    "        if len(predictions) > 0:\n",
    "            if metric == \"f1\":\n",
    "                score = f1_score(valid_true, predictions, average='weighted')\n",
    "            elif metric == \"accuracy\":\n",
    "                score = np.mean(np.array(valid_true) == np.array(predictions))\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_threshold = threshold\n",
    "    \n",
    "    return best_threshold\n",
    "\n",
    "def optimize_class_specific_thresholds(probs: np.ndarray, true_labels: List[int]) -> Dict[int, float]:\n",
    "    \"\"\"\n",
    "    Optimize thresholds for each class to maximize per-class F1 scores.\n",
    "    Uses a proper multi-class decision strategy.\n",
    "    \"\"\"\n",
    "    best_thresholds = {}\n",
    "    threshold_range = np.arange(0.1, 0.9, 0.02)\n",
    "    \n",
    "    for class_id in range(len(label_map)):\n",
    "        best_f1 = 0\n",
    "        best_threshold = 0.5\n",
    "        \n",
    "        for threshold in threshold_range:\n",
    "            # Create binary predictions for this class\n",
    "            binary_preds = (probs[:, class_id] >= threshold).astype(int)\n",
    "            binary_true = (np.array(true_labels) == class_id).astype(int)\n",
    "            \n",
    "            # Calculate F1 for this class\n",
    "            if np.sum(binary_preds) > 0:  # Avoid division by zero\n",
    "                precision = np.sum(binary_preds & binary_true) / np.sum(binary_preds)\n",
    "                recall = np.sum(binary_preds & binary_true) / np.sum(binary_true) if np.sum(binary_true) > 0 else 0\n",
    "                f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "                \n",
    "                if f1 > best_f1:\n",
    "                    best_f1 = f1\n",
    "                    best_threshold = threshold\n",
    "        \n",
    "        best_thresholds[class_id] = best_threshold\n",
    "    \n",
    "    return best_thresholds\n",
    "\n",
    "def predict_with_confidence_threshold(probs: np.ndarray, threshold: float, \n",
    "                                    fallback: str = \"argmax\") -> List:\n",
    "    \"\"\"Make predictions using a confidence threshold\"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for prob in probs:\n",
    "        max_prob = np.max(prob)\n",
    "        if max_prob >= threshold:\n",
    "            predictions.append(np.argmax(prob))\n",
    "        else:\n",
    "            if fallback == \"argmax\":\n",
    "                predictions.append(np.argmax(prob))\n",
    "            elif fallback == \"uncertain\":\n",
    "                predictions.append(-1)  # Use -1 for uncertain\n",
    "            else:\n",
    "                predictions.append(fallback)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def predict_with_class_thresholds(probs: np.ndarray, thresholds: Dict[int, float]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Make predictions using class-specific thresholds.\n",
    "    Strategy: For each sample, check which classes exceed their thresholds,\n",
    "    then pick the one with highest probability among those.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for prob in probs:\n",
    "        # Find classes that exceed their thresholds\n",
    "        confident_classes = []\n",
    "        for class_id, threshold in thresholds.items():\n",
    "            if prob[class_id] >= threshold:\n",
    "                confident_classes.append((class_id, prob[class_id]))\n",
    "        \n",
    "        if confident_classes:\n",
    "            # Pick the class with highest probability among confident classes\n",
    "            confident_classes.sort(key=lambda x: x[1], reverse=True)\n",
    "            predictions.append(confident_classes[0][0])\n",
    "        else:\n",
    "            # Fallback to argmax if no class is confident enough\n",
    "            predictions.append(np.argmax(prob))\n",
    "    \n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5adf074b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading validation data...\n",
      "Loaded 7971 examples\n",
      "Label distribution: [1627 3870 2474]\n",
      "\n",
      "Getting model probabilities...\n",
      "⚠️ Skipping example 1808: 'this idyllic national socialist life could of been' -> index out of range in self\n",
      "⚠️ Skipping example 4943: '#Pedogate #Pizzagate #FollowTheWhiteRabbit RIP Dem' -> index out of range in self\n",
      "⚠️ Skipping example 5104: 'My bitch was so loaded last night &#128514;&#12851' -> index out of range in self\n",
      "⚠️ Skipping example 7895: 'RT @winkSOSA: \"@AintShitSweet__: \"@Rakwon_OGOD: Ni' -> index out of range in self\n",
      "✅ Processed 7967 examples. Skipped 4 problematic examples.\n",
      "\n",
      "==================================================\n",
      "BASELINE (Argmax) Results:\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech      0.594     0.839     0.696      1625\n",
      "   offensive      0.895     0.740     0.810      3868\n",
      "      normal      0.812     0.812     0.812      2474\n",
      "\n",
      "    accuracy                          0.783      7967\n",
      "   macro avg      0.767     0.797     0.773      7967\n",
      "weighted avg      0.808     0.783     0.787      7967\n",
      "\n",
      "\n",
      "==================================================\n",
      "METHOD 1: Confidence Threshold Optimization\n",
      "==================================================\n",
      "Optimal confidence threshold: 0.910\n",
      "\n",
      "Results with confidence threshold:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech      0.594     0.839     0.696      1625\n",
      "   offensive      0.895     0.740     0.810      3868\n",
      "      normal      0.812     0.812     0.812      2474\n",
      "\n",
      "    accuracy                          0.783      7967\n",
      "   macro avg      0.767     0.797     0.773      7967\n",
      "weighted avg      0.808     0.783     0.787      7967\n",
      "\n",
      "Uncertain predictions: 7688 (96.5%)\n",
      "\n",
      "==================================================\n",
      "METHOD 2: Class-Specific Threshold Optimization\n",
      "==================================================\n",
      "Optimal class-specific thresholds:\n",
      "  hatespeech: 0.560\n",
      "  offensive: 0.420\n",
      "  normal: 0.320\n",
      "\n",
      "Results with class-specific thresholds:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  hatespeech      0.624     0.818     0.708      1625\n",
      "   offensive      0.892     0.759     0.820      3868\n",
      "      normal      0.803     0.827     0.815      2474\n",
      "\n",
      "    accuracy                          0.792      7967\n",
      "   macro avg      0.773     0.801     0.781      7967\n",
      "weighted avg      0.810     0.792     0.796      7967\n",
      "\n",
      "\n",
      "==================================================\n",
      "PREDICTION CHANGE ANALYSIS\n",
      "==================================================\n",
      "Predictions changed from baseline:\n",
      "  Confidence threshold: 0 (0.0%)\n",
      "  Class thresholds: 198 (2.5%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load and prepare data\n",
    "print(\"Loading validation data...\")\n",
    "df_val = pd.read_csv(\"../data/val_data.csv\")\n",
    "\n",
    "texts = df_val[\"text\"].tolist()\n",
    "first_label = df_val[\"label\"].iloc[0]\n",
    "if isinstance(first_label, str):\n",
    "    true_labels = [inv_label_map[label] for label in df_val[\"label\"]]\n",
    "else:\n",
    "    true_labels = df_val[\"label\"].tolist()\n",
    "\n",
    "print(f\"Loaded {len(texts)} examples\")\n",
    "print(f\"Label distribution: {np.bincount(true_labels)}\")\n",
    "\n",
    "# Get probabilities\n",
    "print(\"\\nGetting model probabilities...\")\n",
    "val_probs, true_labels = get_probabilities(texts, true_labels, model, tokenizer)\n",
    "\n",
    "# Baseline: Standard argmax predictions\n",
    "baseline_preds = np.argmax(val_probs, axis=1)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BASELINE (Argmax) Results:\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(true_labels, baseline_preds, target_names=list(label_map.values()), digits=3))\n",
    "\n",
    "# Method 1: Confidence threshold optimization\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"METHOD 1: Confidence Threshold Optimization\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "confidence_threshold = optimize_confidence_threshold(val_probs, true_labels, metric=\"f1\")\n",
    "print(f\"Optimal confidence threshold: {confidence_threshold:.3f}\")\n",
    "\n",
    "confidence_preds = predict_with_confidence_threshold(val_probs, confidence_threshold)\n",
    "print(\"\\nResults with confidence threshold:\")\n",
    "print(classification_report(true_labels, confidence_preds, target_names=list(label_map.values()), digits=3))\n",
    "\n",
    "# Show uncertainty analysis\n",
    "uncertain_preds = predict_with_confidence_threshold(val_probs, confidence_threshold, fallback=\"uncertain\")\n",
    "uncertain_count = sum(1 for p in uncertain_preds if p == -1)\n",
    "print(f\"Uncertain predictions: {uncertain_count} ({uncertain_count/len(uncertain_preds)*100:.1f}%)\")\n",
    "\n",
    "# Method 2: Class-specific threshold optimization\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"METHOD 2: Class-Specific Threshold Optimization\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "class_thresholds = optimize_class_specific_thresholds(val_probs, true_labels)\n",
    "print(\"Optimal class-specific thresholds:\")\n",
    "for class_id, threshold in class_thresholds.items():\n",
    "    print(f\"  {label_map[class_id]}: {threshold:.3f}\")\n",
    "\n",
    "class_threshold_preds = predict_with_class_thresholds(val_probs, class_thresholds)\n",
    "print(\"\\nResults with class-specific thresholds:\")\n",
    "print(classification_report(true_labels, class_threshold_preds, target_names=list(label_map.values()), digits=3))\n",
    "\n",
    "# Analysis of prediction changes\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PREDICTION CHANGE ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "baseline_vs_confidence = np.sum(baseline_preds != confidence_preds)\n",
    "baseline_vs_class = np.sum(baseline_preds != class_threshold_preds)\n",
    "\n",
    "print(f\"Predictions changed from baseline:\")\n",
    "print(f\"  Confidence threshold: {baseline_vs_confidence} ({baseline_vs_confidence/len(baseline_preds)*100:.1f}%)\")\n",
    "print(f\"  Class thresholds: {baseline_vs_class} ({baseline_vs_class/len(baseline_preds)*100:.1f}%)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
